<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Project 5 | Congyu Wang </title> <meta name="author" content="Congyu Wang"> <meta name="description" content="Neural Radiance Fields (NeRF)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?554a4c18ed58d926c2409a02f5465304"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wcyalan.github.io/projects/5_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Congyu</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Project 5</h1> <p class="post-description">Neural Radiance Fields (NeRF)</p> </header> <article> <h1 id="neural-radiance-fields-nerf">Neural Radiance Fields (NeRF)</h1> <p>This project implements a Neural Radiance Field (NeRF) to represent and render 3D scenes from multi-view 2D images. Starting with camera calibration and 2D neural fields, we progressively build up to full 3D volumetric rendering.</p> <h2 id="project-overview">Project Overview</h2> <p>Neural Radiance Fields represent 3D scenes as continuous volumetric functions that map 3D coordinates and viewing directions to color and density values. The key components include:</p> <ul> <li> <strong>Camera Calibration</strong>: Recovering intrinsic and extrinsic camera parameters using ArUco markers</li> <li> <strong>2D Neural Fields</strong>: Learning to represent images as continuous functions</li> <li> <strong>3D NeRF</strong>: Volumetric rendering from multi-view images</li> <li> <strong>Custom Dataset</strong>: Training NeRF on self-captured objects</li> </ul> <hr> <h2 id="part-0-camera-calibration-and-3d-scanning">Part 0: Camera Calibration and 3D Scanning</h2> <h3 id="01-camera-calibration">0.1: Camera Calibration</h3> <p>Camera calibration recovers the intrinsic parameters (focal length, principal point) and distortion coefficients of the camera using ArUco markers.</p> <h4 id="calibration-process">Calibration Process</h4> <ol> <li> <strong>Capture Calibration Images</strong>: 30-50 images of ArUco calibration tags from various angles and distances</li> <li> <strong>Detect ArUco Markers</strong>: Use OpenCV’s ArUco detector to find tag corners in each image</li> <li> <strong>Collect Correspondences</strong>: Match 2D image points to 3D world coordinates</li> <li> <strong>Compute Intrinsics</strong>: Use <code class="language-plaintext highlighter-rouge">cv2.calibrateCamera()</code> to recover camera matrix and distortion coefficients</li> </ol> <h4 id="calibration-results">Calibration Results</h4> <div class="row"> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera/1-480.webp 480w,/assets/img/5_project/part0/camera/1-800.webp 800w,/assets/img/5_project/part0/camera/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera/1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Calibration View 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 1</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera/2-480.webp 480w,/assets/img/5_project/part0/camera/2-800.webp 800w,/assets/img/5_project/part0/camera/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera/2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Calibration View 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 2</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera/3-480.webp 480w,/assets/img/5_project/part0/camera/3-800.webp 800w,/assets/img/5_project/part0/camera/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera/3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Calibration View 3" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 3</strong> </div> </div> </div> <div class="row"> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera/12-480.webp 480w,/assets/img/5_project/part0/camera/12-800.webp 800w,/assets/img/5_project/part0/camera/12-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Calibration View 4" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 4</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera/25-480.webp 480w,/assets/img/5_project/part0/camera/25-800.webp 800w,/assets/img/5_project/part0/camera/25-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera/25.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Calibration View 5" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 5</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera/33-480.webp 480w,/assets/img/5_project/part0/camera/33-800.webp 800w,/assets/img/5_project/part0/camera/33-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera/33.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Calibration View 6" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 6</strong> </div> </div> </div> <p><strong>Camera Intrinsics:</strong></p> \[K = \begin{bmatrix} f_x &amp; 0 &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\] <h3 id="02-3d-object-scanning">0.2: 3D Object Scanning</h3> <p>Captured 30-50 images of a chosen object with a single ArUco tag for pose estimation.</p> <div class="row"> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/object/1-480.webp 480w,/assets/img/5_project/part0/object/1-800.webp 800w,/assets/img/5_project/part0/object/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/object/1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scan View 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 1</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/object/3-480.webp 480w,/assets/img/5_project/part0/object/3-800.webp 800w,/assets/img/5_project/part0/object/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/object/3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scan View 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 2</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/object/4-480.webp 480w,/assets/img/5_project/part0/object/4-800.webp 800w,/assets/img/5_project/part0/object/4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/object/4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scan View 3" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 3</strong> </div> </div> </div> <div class="row"> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/object/34-480.webp 480w,/assets/img/5_project/part0/object/34-800.webp 800w,/assets/img/5_project/part0/object/34-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/object/34.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scan View 4" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 4</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/object/44-480.webp 480w,/assets/img/5_project/part0/object/44-800.webp 800w,/assets/img/5_project/part0/object/44-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/object/44.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scan View 5" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 5</strong> </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/object/50-480.webp 480w,/assets/img/5_project/part0/object/50-800.webp 800w,/assets/img/5_project/part0/object/50-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/object/50.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scan View 6" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>View 6</strong> </div> </div> </div> <h3 id="03-camera-pose-estimation">0.3: Camera Pose Estimation</h3> <p>Using the calibrated intrinsics, we estimate camera pose for each image using Perspective-n-Point (PnP).</p> <h4 id="pnp-algorithm">PnP Algorithm</h4> <p>Given 3D-2D correspondences and camera intrinsics, <code class="language-plaintext highlighter-rouge">cv2.solvePnP()</code> recovers the rotation and translation. The algorithm converts the axis-angle rotation vector to a rotation matrix using <code class="language-plaintext highlighter-rouge">cv2.Rodrigues()</code>, then constructs the camera-to-world transformation matrix by inverting the world-to-camera transformation.</p> <h4 id="camera-frustum-visualization">Camera Frustum Visualization</h4> <div class="row"> <div class="col-md-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera_frustums_view1-480.webp 480w,/assets/img/5_project/part0/camera_frustums_view1-800.webp 800w,/assets/img/5_project/part0/camera_frustums_view1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera_frustums_view1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Camera Frustums 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Camera Poses - View 1</strong><br> Estimated camera frustums showing capture positions </div> </div> <div class="col-md-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part0/camera_frustums_view2-480.webp 480w,/assets/img/5_project/part0/camera_frustums_view2-800.webp 800w,/assets/img/5_project/part0/camera_frustums_view2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part0/camera_frustums_view2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Camera Frustums 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Camera Poses - View 2</strong><br> Multiple viewpoints around the object </div> </div> </div> <h3 id="04-image-undistortion-and-dataset-creation">0.4: Image Undistortion and Dataset Creation</h3> <p>Removed lens distortion using <code class="language-plaintext highlighter-rouge">cv2.undistort()</code> and packaged data for NeRF training. Used <code class="language-plaintext highlighter-rouge">cv2.getOptimalNewCameraMatrix()</code> to handle black boundaries from undistortion, cropping images to the valid region of interest and updating the principal point accordingly. The final dataset is saved in <code class="language-plaintext highlighter-rouge">.npz</code> format containing training/validation images, camera poses, and focal length.</p> <hr> <h2 id="part-1-2d-neural-field">Part 1: 2D Neural Field</h2> <p>Before tackling 3D NeRF, we start with a simpler 2D case: representing an image as a continuous neural field (f(x, y) \rightarrow (r, g, b)).</p> <h3 id="11-network-architecture">1.1: Network Architecture</h3> <p><strong>Multilayer Perceptron (MLP) with Positional Encoding:</strong></p> <ul> <li> <strong>Input</strong>: 2D pixel coordinates ((x, y)) normalized to ([0, 1])</li> <li> <strong>Positional Encoding</strong>: Expands input dimensionality using sinusoidal functions</li> <li> <strong>Output</strong>: RGB color values in ([0, 1])</li> </ul> \[\text{PE}(p) = [p, \sin(2^0 \pi p), \cos(2^0 \pi p), \ldots, \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p)]\] <p>where (L) is the maximum frequency level.</p> <p><strong>Network Structure:</strong></p> <div class="row"> <div class="col-md-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/mlp_img-480.webp 480w,/assets/img/5_project/part1/mlp_img-800.webp 800w,/assets/img/5_project/part1/mlp_img-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/mlp_img.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="2D MLP Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>2D Neural Field MLP Architecture</strong><br> Input (x, y) → PE(42-dim) → FC(256) → ReLU → FC(256) → ReLU → FC(256) → ReLU → FC(3) → Sigmoid → RGB </div> </div> </div> <p>The network uses three fully connected layers with ReLU activations, followed by a Sigmoid activation to constrain outputs to valid color range [0, 1].</p> <h3 id="12-training-process">1.2: Training Process</h3> <p><strong>Hyperparameters:</strong></p> <ul> <li>Learning Rate: 1e-2</li> <li>Optimizer: Adam</li> <li>Batch Size: 10,000 pixels per iteration</li> <li>Iterations: 1000-3000</li> <li>Loss: MSE between predicted and ground truth colors</li> </ul> <p><strong>PSNR Metric:</strong></p> \[\text{PSNR} = -10 \cdot \log_{10}(\text{MSE})\] <h3 id="13-training-results">1.3: Training Results</h3> <div class="row"> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/fox-480.webp 480w,/assets/img/5_project/part1/fox-800.webp 800w,/assets/img/5_project/part1/fox-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/fox.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Ground Truth" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Ground Truth</strong> </div> </div> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/neural_image_iter_50-480.webp 480w,/assets/img/5_project/part1/neural_image_iter_50-800.webp 800w,/assets/img/5_project/part1/neural_image_iter_50-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/neural_image_iter_50.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Iteration 50" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 50</strong> </div> </div> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/neural_image_iter_100-480.webp 480w,/assets/img/5_project/part1/neural_image_iter_100-800.webp 800w,/assets/img/5_project/part1/neural_image_iter_100-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/neural_image_iter_100.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Iteration 100" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 100</strong> </div> </div> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/neural_image_iter_500-480.webp 480w,/assets/img/5_project/part1/neural_image_iter_500-800.webp 800w,/assets/img/5_project/part1/neural_image_iter_500-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/neural_image_iter_500.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Iteration 500" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 500</strong> </div> </div> </div> <div class="row"> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/neural_image_iter_1000-480.webp 480w,/assets/img/5_project/part1/neural_image_iter_1000-800.webp 800w,/assets/img/5_project/part1/neural_image_iter_1000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/neural_image_iter_1000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Iteration 1000" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 1000</strong> </div> </div> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/neural_image_iter_3000-480.webp 480w,/assets/img/5_project/part1/neural_image_iter_3000-800.webp 800w,/assets/img/5_project/part1/neural_image_iter_3000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/neural_image_iter_3000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Iteration 3000" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 3000</strong> </div> </div> </div> <div class="row mt-3"> <div class="col-md-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/neural_image_training_curves-480.webp 480w,/assets/img/5_project/part1/neural_image_training_curves-800.webp 800w,/assets/img/5_project/part1/neural_image_training_curves-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/neural_image_training_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="PSNR Curve" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Training PSNR Curve</strong><br> PSNR improves steadily as the network learns the image </div> </div> </div> <h3 id="14-hyperparameter-analysis">1.4: Hyperparameter Analysis</h3> <p><strong>Effect of Positional Encoding Frequency (L):</strong></p> <div class="row"> <div class="col-md-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/ablation_L_comparison-480.webp 480w,/assets/img/5_project/part1/ablation_L_comparison-800.webp 800w,/assets/img/5_project/part1/ablation_L_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/ablation_L_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Frequency Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Positional Encoding Frequency Comparison</strong><br> Different L values affect the network's ability to capture high-frequency details. Lower L values produce blurry results, while higher L values enable sharp reconstruction. </div> </div> </div> <p><strong>Effect of Network Width:</strong></p> <div class="row"> <div class="col-md-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part1/ablation_width_comparison-480.webp 480w,/assets/img/5_project/part1/ablation_width_comparison-800.webp 800w,/assets/img/5_project/part1/ablation_width_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part1/ablation_width_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Width Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Network Width Comparison</strong><br> Network capacity affects reconstruction quality. Narrow networks struggle with complex patterns, while wider networks provide better representation capacity. </div> </div> </div> <hr> <h2 id="part-2-3d-neural-radiance-field">Part 2: 3D Neural Radiance Field</h2> <h3 id="21-ray-generation">2.1: Ray Generation</h3> <h4 id="coordinate-transformations">Coordinate Transformations</h4> <p><strong>Camera to World Transformation:</strong></p> <p>The transformation between camera space and world space is:</p> \[\begin{bmatrix} \mathbf{x}_w \\ 1 \end{bmatrix} = \begin{bmatrix} R &amp; \mathbf{t} \\ \mathbf{0}^T &amp; 1 \end{bmatrix} \begin{bmatrix} \mathbf{x}_c \\ 1 \end{bmatrix}\] <p>where the camera-to-world (c2w) transformation matrix is a 4×4 matrix containing rotation (R) (3×3) and translation (\mathbf{t}) (3×1).</p> <p><strong>Pixel to Camera Coordinates:</strong></p> <p>Given intrinsic matrix (K) and pixel coordinates ((u, v)):</p> \[\mathbf{x}_c = s \cdot K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\] <p><strong>Ray Generation:</strong></p> <p>For each pixel, we compute:</p> <ul> <li> <strong>Ray Origin</strong>: (\mathbf{o} = \mathbf{t}) (camera position in world space)</li> <li> <strong>Ray Direction</strong>: (\mathbf{d} = \frac{R \cdot K^{-1} [u, v, 1]^T}{|R \cdot K^{-1} [u, v, 1]^T|})</li> </ul> <p>The implementation adds 0.5 to pixel coordinates to account for pixel centers, transforms from pixel to camera coordinates using the inverse intrinsic matrix, then to world coordinates using the camera-to-world transformation.</p> <h3 id="22-sampling-strategy">2.2: Sampling Strategy</h3> <p><strong>Sampling Rays:</strong> Randomly sample 10,000 rays per iteration from all training images.</p> <p><strong>Sampling Points Along Rays:</strong></p> <p>For each ray, sample (N) points between near and far planes:</p> \[\mathbf{p}_i = \mathbf{o} + t_i \mathbf{d}, \quad t_i \in [t_{\text{near}}, t_{\text{far}}]\] <p>Stratified sampling with random perturbation during training ensures every location along the ray is explored, preventing overfitting to discrete sample locations.</p> <h3 id="23-rays-and-samples-visualization">2.3: Rays and Samples Visualization</h3> <div class="row"> <div class="col-md-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/rays_cameras-480.webp 480w,/assets/img/5_project/part2/rays_cameras-800.webp 800w,/assets/img/5_project/part2/rays_cameras-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/rays_cameras.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Rays and Cameras" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Camera Frustums with Sampled Rays</strong><br> 100 rays sampled from training cameras </div> </div> </div> <div class="row"> <div class="col-md-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/render-480.webp 480w,/assets/img/5_project/part2/render-800.webp 800w,/assets/img/5_project/part2/render-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/render.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D Samples" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Sample Points in 3D Space</strong><br> Points sampled along rays for volume rendering </div> </div> </div> <h3 id="24-nerf-network-architecture">2.4: NeRF Network Architecture</h3> <p><strong>Input:</strong></p> <ul> <li>3D position (\mathbf{x}) with positional encoding ((L=10))</li> <li>View direction (\mathbf{d}) with positional encoding ((L=4))</li> </ul> <p><strong>Output:</strong></p> <ul> <li>Density (\sigma \geq 0) (ReLU activation)</li> <li>RGB color (\mathbf{c} \in [0, 1]^3) (Sigmoid activation)</li> </ul> <p><strong>Network Structure:</strong></p> <div class="row"> <div class="col-md-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/mlp_nerf-480.webp 480w,/assets/img/5_project/part2/mlp_nerf-800.webp 800w,/assets/img/5_project/part2/mlp_nerf-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/mlp_nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="NeRF MLP Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>NeRF Network Architecture</strong><br> Position PE(63-dim) → FC(256) → ReLU → FC(256) → ReLU → [Concat Position PE] → FC(256) → ReLU → FC(256) → ReLU → FC(256+1) → Density (1) + Feature (256) → [Concat Direction PE(27-dim)] → FC(128) → ReLU → FC(3) → Sigmoid → RGB </div> </div> </div> <p>The network processes position encoding through multiple layers with a skip connection to inject input features in the middle. The density branch uses ReLU activation for non-negative values, while the color branch conditions on view direction and uses Sigmoid for valid RGB range.</p> <h3 id="25-volume-rendering">2.5: Volume Rendering</h3> <p><strong>Continuous Volume Rendering Equation:</strong></p> \[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt\] <p>where transmittance (T(t) = \exp\left(-\int_{t_n}^{t} \sigma(\mathbf{r}(s)) ds\right))</p> <p><strong>Discrete Approximation:</strong></p> \[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i\] <p>where:</p> <ul> <li>(T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)) is the accumulated transmittance</li> <li>(\delta_i = t_{i+1} - t_i) is the distance between samples</li> <li>(\alpha_i = 1 - \exp(-\sigma_i \delta_i)) is the opacity</li> </ul> <p>The implementation computes opacity from density, calculates accumulated transmittance using cumulative product, and performs weighted sum of colors to produce the final rendered pixel color.</p> <h3 id="26-training-results-on-lego-dataset">2.6: Training Results on Lego Dataset</h3> <p><strong>Training Configuration:</strong></p> <ul> <li>Dataset: Lego (200×200 resolution, 100 training views)</li> <li>Batch Size: 10,000 rays per iteration</li> <li>Learning Rate: 5e-4 (Adam optimizer)</li> <li>Iterations: 1000-5000</li> <li>Near/Far Planes: 2.0 / 6.0</li> <li>Samples per Ray: 64</li> </ul> <div class="row"> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/lego_iter_0100-480.webp 480w,/assets/img/5_project/part2/lego_iter_0100-800.webp 800w,/assets/img/5_project/part2/lego_iter_0100-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/lego_iter_0100.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego 100" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 100</strong> </div> </div> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/lego_iter_0500-480.webp 480w,/assets/img/5_project/part2/lego_iter_0500-800.webp 800w,/assets/img/5_project/part2/lego_iter_0500-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/lego_iter_0500.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego 500" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 500</strong> </div> </div> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/lego_iter_1000-480.webp 480w,/assets/img/5_project/part2/lego_iter_1000-800.webp 800w,/assets/img/5_project/part2/lego_iter_1000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/lego_iter_1000.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego 1000" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 1000</strong> </div> </div> <div class="col-md-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/lego_iter_5000-480.webp 480w,/assets/img/5_project/part2/lego_iter_5000-800.webp 800w,/assets/img/5_project/part2/lego_iter_5000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/lego_iter_5000.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego 5000" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 5000</strong> </div> </div> </div> <div class="row mt-3"> <div class="col-md-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/lego_psnr_curve-480.webp 480w,/assets/img/5_project/part2/lego_psnr_curve-800.webp 800w,/assets/img/5_project/part2/lego_psnr_curve-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/lego_psnr_curve.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego PSNR" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Validation PSNR Curve</strong><br> Achieved &gt;23 PSNR on validation set after 1000 iterations </div> </div> </div> <h4 id="novel-view-synthesis">Novel View Synthesis</h4> <div class="row"> <div class="col-md-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/lego_novel_views-480.webp 480w,/assets/img/5_project/part2/lego_novel_views-800.webp 800w,/assets/img/5_project/part2/lego_novel_views-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/lego_novel_views.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego Novel Views" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Novel View Rendering</strong><br> Spherical camera trajectory around the lego bulldozer </div> </div> </div> <hr> <h2 id="part-26-custom-object-nerf">Part 2.6: Custom Object NeRF</h2> <h3 id="dataset-custom-captured-object">Dataset: Custom Captured Object</h3> <p>Trained NeRF on a custom object captured using the calibration pipeline from Part 0.</p> <p><strong>Dataset Statistics:</strong></p> <ul> <li>Training Images: 35</li> <li>Validation Images: 5</li> <li>Image Resolution: 400×300 (resized from original)</li> <li>Near/Far Planes: 0.02 / 0.5 (adjusted for object scale)</li> <li>Samples per Ray: 64</li> </ul> <p><strong>Training Modifications:</strong></p> <ul> <li>Adjusted near/far bounds based on object distance</li> <li>Increased samples per ray from 32 to 64 for better quality</li> <li>Resized images to manage training time</li> <li>Updated intrinsics matrix after resizing</li> </ul> <h3 id="training-progression">Training Progression</h3> <div class="row"> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/custom_iter_0500-480.webp 480w,/assets/img/5_project/part2/custom_iter_0500-800.webp 800w,/assets/img/5_project/part2/custom_iter_0500-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/custom_iter_0500.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Custom 500" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 500</strong><br> Basic shape emerging </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/custom_iter_2000-480.webp 480w,/assets/img/5_project/part2/custom_iter_2000-800.webp 800w,/assets/img/5_project/part2/custom_iter_2000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/custom_iter_2000.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Custom 2000" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 2000</strong><br> Details becoming clearer </div> </div> <div class="col-md-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/custom_iter_6000-480.webp 480w,/assets/img/5_project/part2/custom_iter_6000-800.webp 800w,/assets/img/5_project/part2/custom_iter_6000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/custom_iter_6000.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Custom 6000" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Iteration 6000</strong><br> High-quality reconstruction </div> </div> </div> <h3 id="training-loss-curve">Training Loss Curve</h3> <div class="row"> <div class="col-md-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/training_curves-480.webp 480w,/assets/img/5_project/part2/training_curves-800.webp 800w,/assets/img/5_project/part2/training_curves-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/training_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Custom Loss" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <strong>Training Loss Over Iterations</strong><br> Steady convergence with adjusted hyperparameters </div> </div> </div> <h3 id="novel-view-synthesis-1">Novel View Synthesis</h3> <div class="row"> <div class="col-md-12 mt-3"> <div style="transform: rotate(-90deg); transform-origin: center; width: 100%; display: flex; justify-content: center; align-items: center; min-height: 600px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_project/part2/chips-480.webp 480w,/assets/img/5_project/part2/chips-800.webp 800w,/assets/img/5_project/part2/chips-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5_project/part2/chips.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Custom Novel Views" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> <strong>360° Novel View Rendering</strong><br> Camera circling around the captured object </div> </div> </div> <p><strong>Circular Camera Path Generation:</strong></p> <p>The novel view synthesis uses a circular camera trajectory that looks at the object origin. For each angle, a camera-to-world matrix is computed using the look-at transformation, then rotated around the object. The trained NeRF model renders each view by sampling rays through the scene and applying volume rendering to produce the final images.</p> <hr> <h2 id="key-learnings">Key Learnings</h2> <h3 id="practical-considerations">Practical Considerations</h3> <ul> <li> <p><strong>Near/Far Bounds</strong>: Must be carefully tuned for each scene. Too wide wastes samples in empty space; too narrow clips the object.</p> </li> <li> <p><strong>Image Resolution</strong>: Higher resolution requires more memory and training time. Resizing images is often necessary for practical training.</p> </li> <li> <p><strong>Training Time</strong>: NeRF training is computationally intensive. GPU acceleration is essential, and training can take hours even for small scenes.</p> </li> <li> <p><strong>View-Dependent Effects</strong>: The direction-conditioned color prediction allows NeRF to model specular reflections and view-dependent appearance.</p> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Congyu Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 15, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>